{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle csv upload.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fipeop/ctci-solutions/blob/master/XRD-Colab-V2.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "YxSA6H62QFlN",
        "colab_type": "code",
        "outputId": "de99b9a1-5def-43c1-c6ca-a3414df9d8f3",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8db4b8ac-25ed-4cf8-8bef-030abd261172\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-8db4b8ac-25ed-4cf8-8bef-030abd261172\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving exp_train_x.csv to exp_train_x (1).csv\n",
            "Saving exp_train_y.csv to exp_train_y (1).csv\n",
            "Saving X_exp.csv to X_exp (1).csv\n",
            "Saving X_th.csv to X_th (1).csv\n",
            "Saving y_exp.csv to y_exp.csv\n",
            "Saving y_th.csv to y_th.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gwNGnGMWQiKG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see what this file looks like."
      ]
    },
    {
      "metadata": {
        "id": "Xo_0YkuLQjYe",
        "colab_type": "code",
        "outputId": "c6f2cbf0-9fdf-48ff-a6f2-3b0b42ca8b67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "#Call to bunch of libraries and functions, just run \n",
        "\n",
        "import time  \n",
        "from sklearn import metrics\n",
        "import numpy as np  \n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import os\n",
        "import pandas as pd\n",
        "from scipy.signal import savgol_filter\n",
        "from scipy.signal import find_peaks_cwt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.models import Model\n",
        "import keras as K\n",
        "#from keras models import Sequential\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "##define multiple machine learning algorithms\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "K.backend.clear_session()\n",
        "\n",
        "def next_batch(num, data, labels):\n",
        "    '''\n",
        "    Return a total of `num` random samples and labels. \n",
        "    '''\n",
        "    idx = np.arange(0 , len(data))\n",
        "    np.random.shuffle(idx)\n",
        "    idx = idx[:num]\n",
        "    data_shuffle = [data[ i] for i in idx]\n",
        "    labels_shuffle = [labels[ i] for i in idx]\n",
        "\n",
        "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "# Parameters\n",
        "\n",
        "BATCH_SIZE=256\n",
        "\n",
        "# Network Parameters\n",
        "n_input = 1200 # MNIST data input (img shape: 28*28)\n",
        "n_classes = 7 # MNIST total classes (0-9 digits)\n",
        "#drop_out = 0.2 # Dropout, probability to keep units\n",
        "filter_size = 2\n",
        "kernel_size = 10\n",
        "\n",
        "enc = OneHotEncoder(sparse=False)\n",
        "\n",
        "#def convnet(X_in,n_classes):\n",
        "#   \n",
        "#       \n",
        "#   \n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(K.layers.Conv1D(32, 8, padding='same',input_shape=(1200,1), activation='relu'))\n",
        "model.add(K.layers.Conv1D(32, 5, padding='same', activation='relu'))\n",
        "model.add(K.layers.Conv1D(32, 3, padding='same', activation='relu'))\n",
        "model.add(K.layers.pooling.GlobalAveragePooling1D())\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "#model.add(K.layers.Dense(128, activation='relu'))\n",
        "\n",
        "model.add(K.layers.Dense(n_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "#data normalization from 0 to 1 for double column dataframe, returns single column array\n",
        "def normdata(data):\n",
        "    (len1,w1) = np.shape(data)\n",
        "    ndata = np.zeros([len1,w1//2])\n",
        "    for i in range(w1//2):\n",
        "        ndata[:,i]=(data[:,2*i+1]-min(data[:,2*i+1]))/(max(data[:,2*i+1])-min(data[:,2*i+1]))\n",
        "    return ndata\n",
        "#data normalization from 0 to 1 for single column dataframe\n",
        "def normdatasingle(data):\n",
        "    (len1,w1) = np.shape(data)\n",
        "    ndata = np.zeros([len1,w1])\n",
        "    for i in range(w1):\n",
        "        ndata[:,i]=(data[:,i]-min(data[:,i]))/(max(data[:,i])-min(data[:,i]))\n",
        "    return ndata\n",
        "\n",
        "#data augmendatation for simulated XRD spectrum\n",
        "def augdata(data,num,dframe,minn,maxn):\n",
        "    (len1,w1) = np.shape(data)\n",
        "    augd =np.zeros([len1,num])\n",
        "    naugd=np.zeros([len1,num])\n",
        "    newaugd=np.zeros([len1,num])\n",
        "    crop_augd = np.zeros([maxn-minn,num])\n",
        "    par1 = list(dframe.columns.values)\n",
        "    pard = []\n",
        "    for i in range(num):\n",
        "        rnd = np.random.randint(0,w1)\n",
        "        # create the first filter for peak elimination\n",
        "        dumb= np.repeat(np.random.choice([0,1,1],300),len1//300)\n",
        "        dumb1= np.append(dumb,np.zeros([len1-len(dumb),]))\n",
        "        # create the second filter for peak scaling\n",
        "        dumbrnd= np.repeat(np.random.rand(100,),len1//100)\n",
        "        dumbrnd1=np.append(dumbrnd,np.zeros([len1-len(dumbrnd),]))\n",
        "        #peak eleminsation and scaling\n",
        "        augd[:,i] = np.multiply((data[:,rnd]),dumbrnd1)\n",
        "        augd[:,i] = np.multiply(augd[:,i],dumb1)\n",
        "        #nomrlization\n",
        "        naugd[:,i] = (augd[:,i]-min(augd[:,i]))/(max(augd[:,i])-min(augd[:,i])+1e-9)\n",
        "        pard.append (par1[2*rnd])\n",
        "        #adding shift\n",
        "        cut = np.random.randint(-20*1,20)\n",
        "        #XRD spectrum shift to left\n",
        "        if cut>=0:\n",
        "            newaugd[:,i] = np.append(naugd[cut:,i],np.zeros([cut,]))\n",
        "        #XRD spectrum shift to right\n",
        "        else:\n",
        "            newaugd[:,i] = np.append(naugd[0:len1+cut,i],np.zeros([cut*-1,]))\n",
        "         \n",
        "        crop_augd[:,i] = newaugd[minn:maxn,i]\n",
        "#        \n",
        "    return newaugd, pard,crop_augd\n",
        "#data augmendatation for experimental XRD spectrum\n",
        "def exp_augdata(data,num,label):\n",
        "    (len1,w1) = np.shape(data)\n",
        "    augd =np.zeros([len1,num])\n",
        "    naugd=np.zeros([len1,num])\n",
        "    newaugd=np.zeros([len1,num])\n",
        "    par=np.zeros([num,])\n",
        "    for i in range(num):\n",
        "        rnd = np.random.randint(0,w1)\n",
        "\n",
        "         # create the first filter for peak elimination\n",
        "        dumb= np.repeat(np.random.choice([0,1,1],300),len1//300)\n",
        "        dumb1= np.append(dumb,np.zeros([len1-len(dumb),]))\n",
        "        # create the second filter for peak scaling\n",
        "        dumbrnd= np.repeat(np.random.rand(200,),len1//200)\n",
        "        dumbrnd1=np.append(dumbrnd,np.zeros([len1-len(dumbrnd),]))\n",
        "        #peak eleminsation and scaling\n",
        "        augd[:,i] = np.multiply((data[:,rnd]),dumbrnd1)\n",
        "        augd[:,i] = np.multiply(augd[:,i],dumb1)\n",
        "        #nomrlization\n",
        "        naugd[:,i] = (augd[:,i]-min(augd[:,i]))/(max(augd[:,i])-min(augd[:,i])+1e-9)\n",
        "        par[i,] =label[rnd,] \n",
        "        #adding shift\n",
        "        cut = np.random.randint(-20*1,20)\n",
        "        #XRD spectrum shift to left\n",
        "        if cut>=0:\n",
        "            newaugd[:,i] = np.append(naugd[cut:,i],np.zeros([cut,]))\n",
        "        #XRD spectrum shift to right\n",
        "        else:\n",
        "            newaugd[:,i] = np.append(naugd[0:len1+cut,i],np.zeros([cut*-1,]))\n",
        "\n",
        "    return newaugd, par\n",
        "\n",
        "#extracting exprimental data\n",
        "def exp_data_processing (data,minn,maxn,window):\n",
        "    (len1,w1) = np.shape(data)\n",
        "    nexp1 =np.zeros([maxn-minn,w1])\n",
        "    for i in range(w1):\n",
        "        #savgol_filter to smooth the data\n",
        "         new1 = savgol_filter(data[minn:maxn,i], 31, 3)\n",
        "         #peak finding\n",
        "         zf= find_peaks_cwt(new1, np.arange(10,15), noise_perc=0.01)\n",
        "         #background substraction\n",
        "         for j in range(len(zf)-1):\n",
        "             zf_start= np.maximum(0,zf[j+1]-window//2)\n",
        "             zf_end = np.minimum(zf[j+1]+window//2,maxn)\n",
        "             peak = new1[zf_start:zf_end]\n",
        "             \n",
        "             ##abritaryly remove 1/4 data\n",
        "             npeak = np.maximum(0,peak-max(np.partition(peak,window//5 )[0:window//5]))\n",
        "             nexp1[zf_start:zf_end,i]= npeak     \n",
        "    return nexp1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 1200, 32)          288       \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 1200, 32)          5152      \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 1200, 32)          3104      \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 32)                0         \n",
            "=================================================================\n",
            "Total params: 8,544\n",
            "Trainable params: 8,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CJ9ijZC3Q1Xl",
        "colab_type": "code",
        "outputId": "a00fd065-4c2b-4b20-9a03-4ab6d28a7999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "X_th=pd.read_csv(\"X_th.csv\", index_col='Unnamed: 0')\n",
        "y_th=pd.read_csv(\"y_th.csv\", index_col='Unnamed: 0')\n",
        "y_exp=pd.read_csv(\"y_exp.csv\", index_col='Unnamed: 0')\n",
        "X_exp=pd.read_csv(\"X_exp.csv\", index_col='Unnamed: 0')\n",
        "\n",
        "exp_train_x=pd.read_csv(\"exp_train_x.csv\", index_col='Unnamed: 0')\n",
        "exp_train_y=pd.read_csv(\"exp_train_y.csv\", index_col='Unnamed: 0')\n",
        "\n",
        "#X_exp.shape\n",
        "#y_exp.shape\n",
        "#y_th.shape\n",
        "X_th.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(772, 1200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "4qk0W5_dyDUI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "f49dabcd-7c51-47ac-ed42-435b27b8cee2"
      },
      "cell_type": "code",
      "source": [
        "#data augmentation to experimenal traning dataset\n",
        "#exp_train_x,exp_train_y = exp_augdata(temp_x.T,2000,temp_y)\n",
        "#combine theorectical and experimenal dataset for training\n",
        "train_combine = np.concatenate((X_th,exp_train_x.T))\n",
        "  \n",
        "train_dim = train_combine.reshape(train_combine.shape[0],1200,1)\n",
        "train_y = np.concatenate((y_th,exp_train_y))\n",
        "train_y_hot = enc.fit_transform(train_y.reshape(-1,1))\n",
        "\n",
        "\n",
        "#sess = tf.Session ()\n",
        "#sess.run(tf.global_variables_initializer())\n",
        "\n",
        "      \n",
        "optimizer = K.optimizers.Adam()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor=0.5,\n",
        "                              patience=50, min_lr=0.00001)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "hist = model.fit(train_dim, train_y_hot, batch_size=BATCH_SIZE, nb_epoch=10000,\n",
        "                 verbose=1, callbacks = [reduce_lr])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-9d95af191921>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m hist = model.fit(train_dim, train_y_hot, batch_size=BATCH_SIZE, nb_epoch=10000,\n\u001b[0;32m---> 25\u001b[0;31m                  verbose=1, callbacks = [reduce_lr])\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    802\u001b[0m             ]\n\u001b[1;32m    803\u001b[0m             \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0mcheck_array_length_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m                 \u001b[0;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    235\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                          \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
            "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 2772 input samples and 3000 target samples."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "gry7O9Yk0fxY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('keras_model.h5')        \n",
        "from vis.visualization import visualize_cam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "#ind = np.where(bbb==1)[0]\n",
        "#for j,modifier in enumerate([None, 'guided', 'relu']):\n",
        "#    heat_map = visualize_cam(model, 3, bbb[ind],train_dim[ind], backprop_modifier=modifier)\n",
        "#\n",
        "#    plt.imshow(heat_map)\n",
        "#plt.imshow(heat_map)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# get the weights from the last layer\n",
        "gap_weights = model.layers[-1].get_weights()[0]\n",
        "\n",
        "# create a new model to output the feature maps and the predicted labels\n",
        "cam_model = Model(inputs=model.input, \n",
        "                    outputs=(model.layers[-4].output, model.layers[-1].output)) \n",
        "\n",
        "# make the prediction for a set of test images\n",
        "features, results = cam_model.predict(test_x)\n",
        "\n",
        "# check the prediction for 10 test images\n",
        "for idx in range(7):   \n",
        "    # get the feature map of the test image\n",
        "    features_for_one_img = features[idx, :, :]\n",
        "    \n",
        "\n",
        "    # map the feature map to the original size\n",
        "\n",
        "    cam_features = features_for_one_img\n",
        "        \n",
        "    # get the predicted label with the maximum probability\n",
        "    pred = np.argmax(results[idx])\n",
        "    \n",
        "    # prepare the final display\n",
        "    plt.figure(facecolor='white')\n",
        "    \n",
        "    # get the weights of class activation map\n",
        "    cam_weights = gap_weights[:, pred]\n",
        "\n",
        "    # create the class activation map\n",
        "    cam_output = np.dot(cam_features, cam_weights)\n",
        "    \n",
        "    x = np.linspace(10,60,1200)\n",
        "    y = abs(cam_output)\n",
        "    \n",
        "    fig, (ax,ax2) = plt.subplots(nrows=2, sharex=True)\n",
        "    \n",
        "    extent = [x[0]-(x[1]-x[0])/2., x[-1]+(x[1]-x[0])/2.,0,1]\n",
        "    ax.imshow(y[np.newaxis,:], cmap=\"plasma\", aspect=\"auto\", extent=extent)\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xlim(extent[0], extent[1])\n",
        "    \n",
        "    ax2.plot(x,y)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    ax2.plot(x,y)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}